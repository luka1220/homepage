{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/PAP_Lab4",
    "result": {"data":{"markdownRemark":{"id":"c378c5a7-7c4b-54ad-95f5-559e688e5449","html":"<h1>Introduction</h1>\n<p>The work for this lab presents six different parallel implementations of\nthe CFD (Computational Fluid Dynamic) simulation with <a href=\"https://www.open-mpi.org/\">Open MPI</a> and compares them for weak and strong scaling.\nThere are different methods to simulate a fluid. This project is using an implementation of the Lattice Boltzmann Method (LBM) which is easy to parallelize in distributed memory systems.</p>\n<h1>1D Splitting</h1>\n<p>For the 1D splitting, it is ensured that all sub-domains have the same\nsize, therefore the execution aborts with exit(1) when the total width\nis not dividable by the number of processes. The ranks of the MPI\nprocesses correspond to the sub-domains of the mash from left to right\nin increasing order. The processes communicate the continuous columns on\nthe borders of the domain to their predecessor and successor.</p>\n<p><strong>Exercise 1</strong>.\nTwo cases are distinguished here, the processes of rank 0 have no\ncommunication to the left and process with the highest rank, that is\ncomm_size , to the right. Otherwise, processes receive first from the\nleft and then respond, then the communication to the right starts by\nsending first. As blocking communication is used, this results in a\ndependency for the processes from 0 to comm_size , which should yield\nlower speed-ups. Let us see how it behaves later in section\n<a href=\"#sec:exp\" data-reference-type=\"ref\" data-reference=\"sec:exp\">3</a>.</p>\n<p><strong>Exercise 2</strong>.\nWith an odd-even distinction, the dependency path from Exercise 1 is\ndecreased to the interdependence between processes of odd and even\nranks. First, the processes with an even rank send and then receives\nfrom odd ranks. The edge cases are handled as in Exercise 1.</p>\n<p><strong>Exercise 3</strong>.\nTo further decrease dependency the communication to the left and right\nprocesses is performed here with non-blocking communication. Once all\nfour column exchanges to place independently of each other, the\nsimulation calculation continues, this is ensured with MPI_Waitall()\nafter all non-blocking send and receives.</p>\n<h1>2D Splitting</h1>\n<p>The 2D splitting is performed with MPI_Dims_create that decides on the\nnumber of sub-domains along x and y, such that these are all of equal\nsize. Given the dimensions, the communication card is created and each\nprocess receives its coordinates by the following MPI function, in the\nvariable coords . Where we use the initialized communicator, periods are\ndisabled and the reorder of ranks is allowed.</p>\n<pre><code>// with dimsG = {0,0}; we set no constraints on the dimension\nMPI_Dims_create(comm_size, 2, dimsGiven);\nMPI_Cart_create(MPI_COMM_WORLD, 2, dimsGiven, periodsGiven,\n                            reorderGiven, &#x26;(comm->communicator));\nMPI_Cart_get(comm->communicator, 2, dimsRecv, periodsRecv, coords);\n</code></pre>\n<p>For prim-numbers, this is always one-dimensional. In case that total\nwidth or height is not dividable by the assigned number of processes\nalong the dimension, then all processes are reassigned along the\nx-dimension, and we fall back to 1D.</p>\n<p>Now that vertical row exchange will be implemented, which is\nnon-continues memory here, 4 buffers are allocated to send and receive\nrows of the mash between sub-domains.</p>\n<p><strong>Exercise 4</strong>.\nFor this exercise, the edge cases are handle by MPI_PROC_NULL as target\nrank in the communication. A function is therefore implemented get_rank\nthat resolves the 2D coordinates of the sub-domains to the assigned rank\nwhen the coordinates are outside the mesh, MPI_PROC_NULL is returned.\nAnother function computes the four neighbors of the process. Diagonal\ncommunication is unnecessary because the correct reception of the corner\ncells is ensured by first performing the vertical and then the\nhorizontal communication.</p>\n<p>For readability, the communication is performed by a function that\ndecides for all possibilities providing action (send, recv) and\ndirection (top, bottom, left, right) type. For top and bottom\ncommunication, two functions handle the transfer from the mesh row to\nand from the buffer that is used to send and receive rows. For the\ncommunication odd and even coordinates are distinguished, for x and y\neven receives first and odd sends.</p>\n<p><strong>Exercise 5</strong>.\nMPI provides types for non-continuous memory, that are used here\ninstead, and the perform_action function from exercise 4 is accordingly\nmodified. The code becomes a little easier to read but we should not\nexpect speedups, as MPI will not do any magic.</p>\n<p><strong>Exercise 6</strong>.\nHere the communication is performed non-blocking in two blocks first\nvertical, then horizontal, in between the communication is blocked with\nMPI_Waitall . This way the corner cells are correctly transferred.</p>\n<h1>Experiments: Scalability measurement</h1>\n<p>The Experiments are performed with GCE on a cluster of 8 Instances of\ntype <strong>e2-highcpu-4</strong> that have 4 vCPUs<sup id=\"fnref-1\"><a href=\"#fn-1\" class=\"footnote-ref\">1</a></sup> available. The maximum\nnumber of parallel processes to run is 32.</p>\n<p>Two scalability experiments are carried out for all six exercises.\nFirst, strong scaling on a constant mesh of size <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>980</mn><mo>×</mo><mn>194</mn></mrow><annotation encoding=\"application/x-tex\">980 \\times 194</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">980</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">194</span></span></span></span></span> which\nis the initial mesh of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>400</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">400 \\times 80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">400</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">80</span></span></span></span></span> scaled by a factor of 6, see\nFigure\n<a href=\"#fig:exp\" data-reference-type=\"ref\" data-reference=\"fig:exp\">1</a>\nin the middle. Second, weak scaling. The mesh of <span class=\"math math-inline\"><span class=\"katex\"><span class=\"katex-mathml\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"><semantics><mrow><mn>400</mn><mo>×</mo><mn>80</mn></mrow><annotation encoding=\"application/x-tex\">400 \\times 80</annotation></semantics></math></span><span class=\"katex-html\" aria-hidden=\"true\"><span class=\"base\"><span class=\"strut\" style=\"height:0.72777em;vertical-align:-0.08333em;\"></span><span class=\"mord\">400</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span><span class=\"mbin\">×</span><span class=\"mspace\" style=\"margin-right:0.2222222222222222em;\"></span></span><span class=\"base\"><span class=\"strut\" style=\"height:0.64444em;vertical-align:0em;\"></span><span class=\"mord\">80</span></span></span></span></span> and the\nnumber of processes are scaled jointly by the same factor, illustrated\nin the middle of Figure\n<a href=\"#fig:exp\" data-reference-type=\"ref\" data-reference=\"fig:exp\">1</a>.</p>\n<figure>\n<img src=\"/assets/plots/performance.png\" id=\"fig:exp\" style=\"width:100.0%\" /><figcaption aria-hidden=\"true\"><strong>Left:</strong> Shows the time when scaling the mash and number of processes equally. <strong>Middle:</strong> Running time for different <em>comm size</em> for a mesh of size <span class=\"math inline\">979 × 195</span>. <strong>Right:</strong> Speedups for exercise 3 for the same mesh size.</figcaption>\n</figure>\n<p>The results for exercises 1 and 4 are not displayed as the legend in\nFigure\n<a href=\"#fig:exp\" data-reference-type=\"ref\" data-reference=\"fig:exp\">1</a>\nindicates because they performed equally to exercise 2 and 5,\nrespectively.\nFor the odd-even communication pattern in exercise 2, a speedup was\nexpected compare to exercise 1 because the dependency chain is now\nreduced from comm_size to 2. This is true as much as the last process\nhas to wait in the first iteration for all other processes to finish\ncommunication. But the process of lower rank starts computing after\nfinishing its communication with the next process. Therefore processes\nonly wait on the start-up. Then they are slightly shifted and\nsynchronized so that only in the last iteration the first process stops\nworking before, which is the only drawback. Thus in the meantime, the\ncomputation is fully parallelized, and there is no speedup difference to\nthe odd-even pattern for multiple iterations.</p>\n<p>The results for exercises 4 and 5 the forecast was accurate that the\nMPI_Type_vector is not better than the implemented method to copy from\nand to the buffer for vertical communication.</p>\n<p>Apart from that, there is 1D and 2D Splitting to compare. Figure\n<a href=\"#fig:exp\" data-reference-type=\"ref\" data-reference=\"fig:exp\">1</a>\non the left shows that the blocking and non-blocking implementation of\n1D splitting is faster than this 2D splitting counterpart. That relates\nto the fact that the communication in 1D happens in continuous memory,\nwhich is faster because of the extra copy operation for non-continuous\nmemory. In addition, is the number of communicated cells mostly greater\nin the 2D splitting. It was shown that the 1D splitting communicates (for 32 processes) a height\nof 450, and 2D splitting a height plus width of 400, where 286 cells are\nnon-continuous memory. The 2D splitting could be further optimized,\ntrying to keep the width shorter or equal to the height, minimizing\ncontinuous communication (width) and circumference of the sub-domain.\nSuch an approach is not part of this work, as its advantages are\nlimited.</p>\n<p>The non-blocking communication in exercises 3 and 6 shows its advantage\nmore explicitly. The 1D splitting remains faster, besides the time spent\non copying cells for exercise 6, it has to communicate the two\ndirections vertical and horizontal communication sequentially, to ensure\ncorrect calculations for the corners of the sub-domains.</p>\n<p>Overall the parallelization of the CFD simulation is successful, and the\nspeedups are close to the number of processes, see Figure\n<a href=\"#fig:exp\" data-reference-type=\"ref\" data-reference=\"fig:exp\">1</a>\nin the middle and on the right for the speedups for exercise 3. One\nobservation that is pointing out here, is that from 16 to 14 processes,\nand indeed it is the same for 17 processes, the running time almost\ndoubles, and the speedup for 32 processes is just above 14. This\nbehavior indicates that there are some shared resources on the\ninstances, that are only available twice. Hence the third process is\nthen blocked, which blocks, in consequence, the other processes.\nNotably, each MPI process runs on a virtual CPU where one core is\nhyperthreaded, that compute in parallel but shares resources such a\nmemory<sup id=\"fnref-2\"><a href=\"#fn-2\" class=\"footnote-ref\">2</a></sup>.</p>\n<h1>Conclusion</h1>\n<p>The different implementations have illustrated various strategies to\nparallelize simulations. Even though the 1D non-blocking implementation\noutperformed the other tactics for the presented experiments, for more\nextensive simulations with more than 64 cores, the 2D splitting can\nreduce the communication significantly, and advantages can be\nanticipated with vast confidence.</p>\n<img src=\"assets/plots/output.gif\">\n<div class=\"footnotes\">\n<hr>\n<ol>\n<li id=\"fn-1\">\"On Compute Engine, each virtual CPU (vCPU) is implemented as a\nsingle hardware multithread on one of the available CPU processors.\"\nfrom <a href=\"https://cloud.google.com/compute/docs/cpu-platforms\">https://cloud.google.com/compute/docs/cpu-platforms</a><a href=\"#fnref-1\" class=\"footnote-backref\">↩</a></li>\n<li id=\"fn-2\"><a href=\"https://en.wikipedia.org/wiki/Hyper-threading\">https://en.wikipedia.org/wiki/Hyper-threading</a><a href=\"#fnref-2\" class=\"footnote-backref\">↩</a></li>\n</ol>\n</div>","excerpt":"Introduction The work for this lab presents six different parallel implementations of\nthe CFD (Computational Fluid Dynamic) simulation with Open MPI…","frontmatter":{"date":"April 27, 2021","slug":"/PAP_Lab4","title":"Parallel Programming for CFD simulation","description":"Parallel Algorithms and Programming LAB 4","featuredImage":{"childImageSharp":{"gatsbyImageData":{"layout":"fullWidth","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/605457f5d06554199b9f186776500ddd/83a18/animate_43.png","srcSet":"/static/605457f5d06554199b9f186776500ddd/c20f3/animate_43.png 750w,\n/static/605457f5d06554199b9f186776500ddd/809b5/animate_43.png 1080w,\n/static/605457f5d06554199b9f186776500ddd/e6f4d/animate_43.png 1366w,\n/static/605457f5d06554199b9f186776500ddd/83a18/animate_43.png 1920w","sizes":"100vw"},"sources":[{"srcSet":"/static/605457f5d06554199b9f186776500ddd/b7b89/animate_43.webp 750w,\n/static/605457f5d06554199b9f186776500ddd/642fb/animate_43.webp 1080w,\n/static/605457f5d06554199b9f186776500ddd/793de/animate_43.webp 1366w,\n/static/605457f5d06554199b9f186776500ddd/53804/animate_43.webp 1920w","type":"image/webp","sizes":"100vw"}]},"width":1,"height":0.24375000000000002}}}}}},"pageContext":{"id":"c378c5a7-7c4b-54ad-95f5-559e688e5449","previous":{"id":"e6d028ef-359c-583a-a7f9-89407f3240c7","frontmatter":{"slug":"/PAP_Lab2","template":"blog-post","title":"Parallel Programming for Sorting Algorithms"}},"next":{"id":"9b1014c1-bfd9-51cf-af9a-b3e284d8f53d","frontmatter":{"slug":"/about","template":"about-page","title":"About"}}}},
    "staticQueryHashes": ["228695001","2744905544","3050756635"]}